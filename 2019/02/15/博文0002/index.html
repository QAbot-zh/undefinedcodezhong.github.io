<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>博文0002 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Yolo-v3 and Yolo-v2 for Linux参考链接转载于github, 将来闲暇补充翻译 Requirements Pre-trained models Explanations in issues   Improvements in this repository How to use How to compile on Linux How to compile on Windo">
<meta name="keywords" content="yolo-darknet">
<meta property="og:type" content="article">
<meta property="og:title" content="博文0002">
<meta property="og:url" content="http://yoursite.com/2019/02/15/博文0002/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Yolo-v3 and Yolo-v2 for Linux参考链接转载于github, 将来闲暇补充翻译 Requirements Pre-trained models Explanations in issues   Improvements in this repository How to use How to compile on Linux How to compile on Windo">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://pjreddie.com/media/files/darknet-black-small.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/4096485/52151356-e5d4a380-2683-11e9-9d7d-ac7bc192c477.jpg">
<meta property="og:image" content="http://img.youtube.com/vi/VOC3huqHrss/0.jpg">
<meta property="og:image" content="https://hsto.org/files/5dc/7ae/7fa/5dc7ae7fad9d4e3eb3a484c58bfc1ff5.png">
<meta property="og:image" content="https://hsto.org/webt/yd/vl/ag/ydvlagutof2zcnjodstgroen8ac.jpeg">
<meta property="og:image" content="https://hsto.org/files/ca8/866/d76/ca8866d76fb840228940dbf442a7f06a.jpg">
<meta property="og:image" content="https://hsto.org/files/d12/1e7/515/d121e7515f6a4eb694913f10de5f2b61.jpg">
<meta property="og:image" content="https://hsto.org/files/727/c7e/5e9/727c7e5e99bf4d4aa34027bb6a5e4bab.jpg">
<meta property="og:updated_time" content="2019-02-15T08:02:40.878Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="博文0002">
<meta name="twitter:description" content="Yolo-v3 and Yolo-v2 for Linux参考链接转载于github, 将来闲暇补充翻译 Requirements Pre-trained models Explanations in issues   Improvements in this repository How to use How to compile on Linux How to compile on Windo">
<meta name="twitter:image" content="http://pjreddie.com/media/files/darknet-black-small.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-博文0002" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/02/15/博文0002/" class="article-date">
  <time datetime="2019-02-15T05:15:33.000Z" itemprop="datePublished">2019-02-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      博文0002
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Yolo-v3-and-Yolo-v2-for-Linux"><a href="#Yolo-v3-and-Yolo-v2-for-Linux" class="headerlink" title="Yolo-v3 and Yolo-v2 for Linux"></a>Yolo-v3 and Yolo-v2 for Linux</h1><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><a href="https://github.com/AlexeyAB/darknet#how-to-compile-on-linux" target="_blank" rel="noopener">参考链接</a></h3><h4 id="转载于github-将来闲暇补充翻译"><a href="#转载于github-将来闲暇补充翻译" class="headerlink" title="转载于github, 将来闲暇补充翻译"></a>转载于github, 将来闲暇补充翻译</h4><ul>
<li><a href="#requirements">Requirements</a></li>
<li><a href="#pre-trained-models">Pre-trained models</a></li>
<li><a href="https://github.com/AlexeyAB/darknet/issues?q=is%3Aopen+is%3Aissue+label%3AExplanations" target="_blank" rel="noopener">Explanations in issues</a></li>
</ul>
<ol start="0">
<li><a href="#improvements-in-this-repository">Improvements in this repository</a></li>
<li><a href="#how-to-use">How to use</a></li>
<li><a href="#how-to-compile-on-linux">How to compile on Linux</a></li>
<li><a href="#how-to-compile-on-windows">How to compile on Windows</a></li>
<li><a href="#how-to-train-pascal-voc-data">How to train (Pascal VOC Data)</a></li>
<li><a href="#how-to-train-to-detect-your-custom-objects">How to train (to detect your custom objects)</a></li>
<li><a href="#when-should-i-stop-training">When should I stop training</a></li>
<li><a href="#how-to-calculate-map-on-pascalvoc-2007">How to calculate mAP on PascalVOC 2007</a></li>
<li><a href="#how-to-improve-object-detection">How to improve object detection</a></li>
<li><a href="#how-to-mark-bounded-boxes-of-objects-and-create-annotation-files">How to mark bounded boxes of objects and create annotation files</a></li>
<li><a href="#using-yolo9000">Using Yolo9000</a></li>
<li><a href="#how-to-use-yolo-as-dll-and-so-libraries">How to use Yolo as DLL and SO libraries</a></li>
</ol>
<table>
<thead>
<tr>
<th><img src="http://pjreddie.com/media/files/darknet-black-small.png" alt="Darknet Logo"></th>
<th>&nbsp; <img src="https://user-images.githubusercontent.com/4096485/52151356-e5d4a380-2683-11e9-9d7d-ac7bc192c477.jpg" alt="map_time"> <a href="mailto:mAP@0.5" target="_blank" rel="noopener">mAP@0.5</a> (AP50) <a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="noopener">https://pjreddie.com/media/files/papers/YOLOv3.pdf</a></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li><em>YOLOv3-spp</em> 优于 YOLOv3 - mAP = 60.6%, FPS = 20: <a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">https://pjreddie.com/darknet/yolo/</a></li>
<li>Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): <a href="https://arxiv.org/pdf/1708.02002.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1708.02002.pdf</a></li>
<li>Yolo v2 on Pascal VOC 2007: <a href="https://hsto.org/files/a24/21e/068/a2421e0689fb43f08584de9d44c2215f.jpg" target="_blank" rel="noopener">https://hsto.org/files/a24/21e/068/a2421e0689fb43f08584de9d44c2215f.jpg</a></li>
<li>Yolo v2 on Pascal VOC 2012 (comp4): <a href="https://hsto.org/files/3a6/fdf/b53/3a6fdfb533f34cee9b52bdd9bb0b19d9.jpg" target="_blank" rel="noopener">https://hsto.org/files/3a6/fdf/b53/3a6fdfb533f34cee9b52bdd9bb0b19d9.jpg</a></li>
</ul>
<h1 id="“You-Only-Look-Once-Unified-Real-Time-Object-Detection-versions-2-amp-3-”"><a href="#“You-Only-Look-Once-Unified-Real-Time-Object-Detection-versions-2-amp-3-”" class="headerlink" title="“You Only Look Once: Unified, Real-Time Object Detection (versions 2 &amp; 3)”"></a>“You Only Look Once: Unified, Real-Time Object Detection (versions 2 &amp; 3)”</h1><p>A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: <a href="https://github.com/AlexeyAB/darknet/graphs/contributors" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/graphs/contributors</a><br>作者的开源支持包括windows和linux版本</p>
<p>This repository is forked from Linux-version: <a href="https://github.com/pjreddie/darknet" target="_blank" rel="noopener">https://github.com/pjreddie/darknet</a></p>
<p>More details: <a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">http://pjreddie.com/darknet/yolo/</a></p>
<p>This repository supports:</p>
<ul>
<li>both Windows and Linux</li>
<li>both OpenCV 2.x.x and OpenCV &lt;= 3.4.0 (3.4.1 and higher isn’t supported, but you can try)</li>
<li>both cuDNN &gt;= v7</li>
<li>CUDA &gt;= 7.5</li>
<li>also create SO-library on Linux and DLL-library on Windows</li>
</ul>
<h5 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements:"></a>Requirements:</h5><ul>
<li><strong>Linux GCC&gt;=4.9 or Windows MS Visual Studio 2015 (v140)</strong>: <a href="https://go.microsoft.com/fwlink/?LinkId=532606&amp;clcid=0x409" target="_blank" rel="noopener">https://go.microsoft.com/fwlink/?LinkId=532606&amp;clcid=0x409</a>  (or offline <a href="https://go.microsoft.com/fwlink/?LinkId=615448&amp;clcid=0x409" target="_blank" rel="noopener">ISO image</a>)</li>
<li><strong>CUDA 10.0</strong>: <a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a> (on Linux do <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#post-installation-actions" target="_blank" rel="noopener">Post-installation Actions</a>)</li>
<li><strong>OpenCV 3.3.0</strong>: <a href="https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.3.0/opencv-3.3.0-vc14.exe/download" target="_blank" rel="noopener">https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.3.0/opencv-3.3.0-vc14.exe/download</a></li>
<li><strong>or OpenCV 2.4.13</strong>: <a href="https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download" target="_blank" rel="noopener">https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download</a><ul>
<li>OpenCV allows to show image or video detection in the window and store result to file that specified in command line <code>-out_filename res.avi</code></li>
</ul>
</li>
<li><strong>GPU with CC &gt;= 3.0</strong>: <a href="https://en.wikipedia.org/wiki/CUDA#GPUs_supported" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/CUDA#GPUs_supported</a></li>
</ul>
<h5 id="Pre-trained-models"><a href="#Pre-trained-models" class="headerlink" title="Pre-trained models"></a>Pre-trained models</h5><p>预训练模型下载网址：<br>There are weights-file for different cfg-files (smaller size -&gt; faster speed &amp; lower accuracy:</p>
<ul>
<li><code>yolov3-openimages.cfg</code> (247 MB COCO <strong>Yolo v3</strong>) - requires 4 GB GPU-RAM: <a href="https://pjreddie.com/media/files/yolov3-openimages.weights" target="_blank" rel="noopener">https://pjreddie.com/media/files/yolov3-openimages.weights</a></li>
<li><code>yolov3-spp.cfg</code> (240 MB COCO <strong>Yolo v3</strong>) - requires 4 GB GPU-RAM: <a href="https://pjreddie.com/media/files/yolov3-spp.weights" target="_blank" rel="noopener">https://pjreddie.com/media/files/yolov3-spp.weights</a></li>
<li><code>yolov3.cfg</code> (236 MB COCO <strong>Yolo v3</strong>) - requires 4 GB GPU-RAM: <a href="https://pjreddie.com/media/files/yolov3.weights" target="_blank" rel="noopener">https://pjreddie.com/media/files/yolov3.weights</a></li>
<li><code>yolov3-tiny.cfg</code> (34 MB COCO <strong>Yolo v3 tiny</strong>) - requires 1 GB GPU-RAM:  <a href="https://pjreddie.com/media/files/yolov3-tiny.weights" target="_blank" rel="noopener">https://pjreddie.com/media/files/yolov3-tiny.weights</a></li>
<li><code>yolov2.cfg</code> (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: <a href="https://pjreddie.com/media/files/yolov2.weights" target="_blank" rel="noopener">https://pjreddie.com/media/files/yolov2.weights</a></li>
<li><code>yolo-voc.cfg</code> (194 MB VOC Yolo v2) - requires 4 GB GPU-RAM: <a href="http://pjreddie.com/media/files/yolo-voc.weights" target="_blank" rel="noopener">http://pjreddie.com/media/files/yolo-voc.weights</a></li>
<li><code>yolov2-tiny.cfg</code> (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: <a href="https://pjreddie.com/media/files/yolov2-tiny.weights" target="_blank" rel="noopener">https://pjreddie.com/media/files/yolov2-tiny.weights</a></li>
<li><code>yolov2-tiny-voc.cfg</code> (60 MB VOC Yolo v2) - requires 1 GB GPU-RAM: <a href="http://pjreddie.com/media/files/yolov2-tiny-voc.weights" target="_blank" rel="noopener">http://pjreddie.com/media/files/yolov2-tiny-voc.weights</a></li>
<li><code>yolo9000.cfg</code> (186 MB Yolo9000-model) - requires 4 GB GPU-RAM: <a href="http://pjreddie.com/media/files/yolo9000.weights" target="_blank" rel="noopener">http://pjreddie.com/media/files/yolo9000.weights</a></li>
</ul>
<p>Put it near compiled: darknet.exe</p>
<p>You can get cfg-files by path: <code>darknet/cfg/</code></p>
<h5 id="Examples-of-results"><a href="#Examples-of-results" class="headerlink" title="Examples of results:"></a>Examples of results:</h5><p><a href="https://www.youtube.com/watch?v=VOC3huqHrss" title="Everything Is AWESOME" target="_blank" rel="noopener"><img src="http://img.youtube.com/vi/VOC3huqHrss/0.jpg" alt="Everything Is AWESOME"></a></p>
<p>Others: <a href="https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg" target="_blank" rel="noopener">https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg</a></p>
<h3 id="Improvements-in-this-repository"><a href="#Improvements-in-this-repository" class="headerlink" title="Improvements in this repository"></a>Improvements in this repository</h3><ul>
<li>added support for Windows</li>
<li>improved binary neural network performance <strong>2x-4x times</strong> for Detection on CPU and GPU if you trained your own weights by using this XNOR-net model (bit-1 inference) : <a href="https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov3-tiny_xnor.cfg" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov3-tiny_xnor.cfg</a></li>
<li>improved neural network performance <strong>~7%</strong> by fusing 2 layers into 1: Convolutional + Batch-norm</li>
<li>improved neural network performance Detection <strong>3x times</strong>, Training <strong>2 x times</strong> on GPU Volta (Tesla V100, Titan V, …) using Tensor Cores if <code>CUDNN_HALF</code> defined in the <code>Makefile</code> or <code>darknet.sln</code></li>
<li>improved performance <strong>~1.2x</strong> times on FullHD, <strong>~2x</strong> times on 4K, for detection on the video (file/stream) using <code>darknet detector demo</code>… </li>
<li>improved performance <strong>3.5 X times</strong> of data augmentation for training (using OpenCV SSE/AVX functions instead of hand-written functions) - removes bottleneck for training on multi-GPU or GPU Volta</li>
<li>improved performance of detection and training on Intel CPU with AVX (Yolo v3 <strong>~85%</strong>, Yolo v2 ~10%)</li>
<li>fixed usage of <code>[reorg]</code>-layer</li>
<li>optimized memory allocation during network resizing when <code>random=1</code></li>
<li>optimized initialization GPU for detection - we use batch=1 initially instead of re-init with batch=1</li>
<li>added correct calculation of <strong>mAP, F1, IoU, Precision-Recall</strong> using command <code>darknet detector map</code>…</li>
<li>added drawing of chart of average-Loss and accuracy-mAP (<code>-map</code> flag) during training</li>
<li>run <code>./darknet detector demo ... -json_port 8070 -mjpeg_port 8090</code> as JSON and MJPEG server to get results online over the network by using your soft or Web-browser</li>
<li>added calculation of anchors for training</li>
<li>added example of Detection and Tracking objects: <a href="https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp</a></li>
<li>fixed code for use Web-cam on OpenCV 3.x</li>
<li>run-time tips and warnings if you use incorrect cfg-file or dataset</li>
<li>many other fixes of code…</li>
</ul>
<p>And added manual - <a href="#how-to-train-to-detect-your-custom-objects">How to train Yolo v3/v2 (to detect your custom objects)</a></p>
<p>Also, you might be interested in using a simplified repository where is implemented INT8-quantization (+30% speedup and -1% mAP reduced): <a href="https://github.com/AlexeyAB/yolo2_light" target="_blank" rel="noopener">https://github.com/AlexeyAB/yolo2_light</a></p>
<h3 id="How-to-use"><a href="#How-to-use" class="headerlink" title="How to use:"></a>How to use:</h3><p>使用说明</p>
<h5 id="How-to-use-on-the-command-line"><a href="#How-to-use-on-the-command-line" class="headerlink" title="How to use on the command line:"></a>How to use on the command line:</h5><p>On Linux use <code>./darknet</code> instead of <code>darknet.exe</code>, like this:<code>./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights</code></p>
<p>On Linux find executable file <code>./darknet</code> in the root directory, while on Windows find it in the directory <code>\build\darknet\x64</code> </p>
<ul>
<li>Yolo v3 COCO - <strong>image</strong>: <code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25</code></li>
<li><strong>Output coordinates</strong> of objects: <code>darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg</code></li>
<li>Yolo v3 COCO - <strong>video</strong>: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4</code></li>
<li>Yolo v3 COCO - <strong>WebCam 0</strong>: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0</code></li>
<li>Yolo v3 COCO for <strong>net-videocam</strong> - Smart WebCam: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg</code></li>
<li>Yolo v3 - <strong>save result videofile res.avi</strong>: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 test.mp4 -out_filename res.avi</code></li>
<li>Yolo v3 <strong>Tiny</strong> COCO - video: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4</code></li>
<li><strong>JSON and MJPEG server</strong> that allows multiple connections from your soft or Web-browser <code>ip-address:8070</code> and 8090: <code>./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output</code></li>
<li>Yolo v3 Tiny <strong>on GPU #0</strong>: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 0 test.mp4</code></li>
<li>Alternative method Yolo v3 COCO - image: <code>darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25</code></li>
<li>Train on <strong>Amazon EC2</strong>, to see mAP &amp; Loss-chart using URL like: <code>http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090</code> in the Chrome/Firefox:<br>  <code>./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map</code></li>
<li>186 MB Yolo9000 - image: <code>darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights</code></li>
<li>Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app</li>
<li>To process a list of images <code>data/train.txt</code> and save results of detection to <code>result.txt</code> use:<br>  <code>darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -dont_show -ext_output &lt; data/train.txt &gt; result.txt</code></li>
<li>Pseudo-lableing - to process a list of images <code>data/new_train.txt</code> and save results of detection in Yolo training format for each image as label <code>&lt;image_name&gt;.txt</code> (in this way you can increase the amount of training data) use:<br>  <code>darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels &lt; data/new_train.txt</code></li>
<li>To calculate anchors: <code>darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416</code></li>
<li>To check accuracy mAP@IoU=50: <code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights</code></li>
<li>To check accuracy mAP@IoU=75: <code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75</code></li>
</ul>
<h5 id="For-using-network-video-camera-mjpeg-stream-with-any-Android-smartphone"><a href="#For-using-network-video-camera-mjpeg-stream-with-any-Android-smartphone" class="headerlink" title="For using network video-camera mjpeg-stream with any Android smartphone:"></a>For using network video-camera mjpeg-stream with any Android smartphone:</h5><ol>
<li>Download for Android phone mjpeg-stream soft: IP Webcam / Smart WebCam</li>
</ol>
<pre><code>* Smart WebCam - preferably: https://play.google.com/store/apps/details?id=com.acontech.android.SmartWebCam2
* IP Webcam: https://play.google.com/store/apps/details?id=com.pas.webcam
</code></pre><ol start="2">
<li>Connect your Android phone to computer by WiFi (through a WiFi-router) or USB</li>
<li>Start Smart WebCam on your phone</li>
<li>Replace the address below, on shown in the phone application (Smart WebCam) and launch:</li>
</ol>
<ul>
<li>Yolo v3 COCO-model: <code>darknet.exe detector demo data/coco.data yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0</code></li>
</ul>
<h3 id="How-to-compile-on-Linux"><a href="#How-to-compile-on-Linux" class="headerlink" title="How to compile on Linux:"></a>How to compile on Linux:</h3><p>Just do <code>make</code> in the darknet directory.<br>Before make, you can set such options in the <code>Makefile</code>: <a href="https://github.com/AlexeyAB/darknet/blob/9c1b9a2cf6363546c152251be578a21f3c3caec6/Makefile#L1" target="_blank" rel="noopener">link</a></p>
<ul>
<li><code>GPU=1</code> to build with CUDA to accelerate by using GPU (CUDA should be in <code>/usr/local/cuda</code>)</li>
<li><code>CUDNN=1</code> to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in <code>/usr/local/cudnn</code>)</li>
<li><code>CUDNN_HALF=1</code> to build for Tensor Cores (on Titan V / Tesla V100 / DGX-2 and later) speedup Detection 3x, Training 2x</li>
<li><code>OPENCV=1</code> to build with OpenCV 3.x/2.4.x - allows to detect on video files and video streams from network cameras or web-cams</li>
<li><code>DEBUG=1</code> to bould debug version of Yolo</li>
<li><code>OPENMP=1</code> to build with OpenMP support to accelerate Yolo by using multi-core CPU</li>
<li><code>LIBSO=1</code> to build a library <code>darknet.so</code> and binary runable file <code>uselib</code> that uses this library. Or you can try to run so <code>LD_LIBRARY_PATH=./:$LD_LIBRARY_PATH ./uselib test.mp4</code> How to use this SO-library from your own code - you can look at C++ example: <a href="https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp</a><br>  or use in such a way: <code>LD_LIBRARY_PATH=./:$LD_LIBRARY_PATH ./uselib data/coco.names cfg/yolov3.cfg yolov3.weights test.mp4</code></li>
</ul>
<p>To run Darknet on Linux use examples from this article, just use <code>./darknet</code> instead of <code>darknet.exe</code>, i.e. use this command: <code>./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights</code></p>
<h3 id="How-to-compile-on-Windows"><a href="#How-to-compile-on-Windows" class="headerlink" title="How to compile on Windows:"></a>How to compile on Windows:</h3><ol>
<li><p>If you have <strong>MSVS 2015, CUDA 10.0, cuDNN 7.4 and OpenCV 3.x</strong> (with paths: <code>C:\opencv_3.0\opencv\build\include</code> &amp; <code>C:\opencv_3.0\opencv\build\x64\vc14\lib</code>), then start MSVS, open <code>build\darknet\darknet.sln</code>, set <strong>x64</strong> and <strong>Release</strong> <a href="https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg" target="_blank" rel="noopener">https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg</a> and do the: Build -&gt; Build darknet. Also add Windows system variable <code>cudnn</code> with path to CUDNN: <a href="https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg" target="_blank" rel="noopener">https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg</a> <strong>NOTE:</strong> If installing OpenCV, use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see <a href="https://github.com/AlexeyAB/darknet/issues/500" target="_blank" rel="noopener">#500</a>).</p>
<p> 1.1. Find files <code>opencv_world320.dll</code> and <code>opencv_ffmpeg320_64.dll</code> (or <code>opencv_world340.dll</code> and <code>opencv_ffmpeg340_64.dll</code>) in <code>C:\opencv_3.0\opencv\build\x64\vc14\bin</code> and put it near with <code>darknet.exe</code></p>
<p> 1.2 Check that there are <code>bin</code> and <code>include</code> folders in the <code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0</code> if aren’t, then copy them to this folder from the path where is CUDA installed</p>
<p> 1.3. To install CUDNN (speedup neural network), do the following:</p>
<ul>
<li><p>download and install <strong>cuDNN v7.4.1 for CUDA 10.0</strong>: <a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-archive</a></p>
</li>
<li><p>add Windows system variable <code>cudnn</code> with path to CUDNN: <a href="https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg" target="_blank" rel="noopener">https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg</a></p>
</li>
<li><p>copy file <code>cudnn64_7.dll</code> to the folder <code>\build\darknet\x64</code> near with <code>darknet.exe</code></p>
<p>1.4. If you want to build <strong>without CUDNN</strong> then: open <code>\darknet.sln</code> -&gt; (right click on project) -&gt; properties  -&gt; C/C++ -&gt; Preprocessor -&gt; Preprocessor Definitions, and remove this: <code>CUDNN;</code></p>
</li>
</ul>
</li>
<li><p>If you have other version of <strong>CUDA (not 10.0)</strong> then open <code>build\darknet\darknet.vcxproj</code> by using Notepad, find 2 places with “CUDA 10.0” and change it to your CUDA-version, then do step 1</p>
</li>
<li><p>If you <strong>don’t have GPU</strong>, but have <strong>MSVS 2015 and OpenCV 3.0</strong> (with paths: <code>C:\opencv_3.0\opencv\build\include</code> &amp; <code>C:\opencv_3.0\opencv\build\x64\vc14\lib</code>), then start MSVS, open <code>build\darknet\darknet_no_gpu.sln</code>, set <strong>x64</strong> and <strong>Release</strong>, and do the: Build -&gt; Build darknet_no_gpu</p>
</li>
<li><p>If you have <strong>OpenCV 2.4.13</strong> instead of 3.0 then you should change pathes after <code>\darknet.sln</code> is opened</p>
<p> 4.1 (right click on project) -&gt; properties  -&gt; C/C++ -&gt; General -&gt; Additional Include Directories:  <code>C:\opencv_2.4.13\opencv\build\include</code></p>
<p> 4.2 (right click on project) -&gt; properties  -&gt; Linker -&gt; General -&gt; Additional Library Directories: <code>C:\opencv_2.4.13\opencv\build\x64\vc14\lib</code></p>
</li>
<li><p>If you have GPU with Tensor Cores (nVidia Titan V / Tesla V100 / DGX-2 and later) speedup Detection 3x, Training 2x:<br> <code>\darknet.sln</code> -&gt; (right click on project) -&gt; properties -&gt; C/C++ -&gt; Preprocessor -&gt; Preprocessor Definitions, and add here: <code>CUDNN_HALF;</code></p>
<p> <strong>Note:</strong> CUDA must be installed only after that MSVS2015 had been installed.</p>
</li>
</ol>
<h3 id="How-to-compile-custom"><a href="#How-to-compile-custom" class="headerlink" title="How to compile (custom):"></a>How to compile (custom):</h3><p>Also, you can to create your own <code>darknet.sln</code> &amp; <code>darknet.vcxproj</code>, this example for CUDA 9.1 and OpenCV 3.0</p>
<p>Then add to your created project:</p>
<ul>
<li>(right click on project) -&gt; properties  -&gt; C/C++ -&gt; General -&gt; Additional Include Directories, put here: </li>
</ul>
<p><code>C:\opencv_3.0\opencv\build\include;..\..\3rdparty\include;%(AdditionalIncludeDirectories);$(CudaToolkitIncludeDir);$(cudnn)\include</code></p>
<ul>
<li>(right click on project) -&gt; Build dependecies -&gt; Build Customizations -&gt; set check on CUDA 9.1 or what version you have - for example as here: <a href="http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg" target="_blank" rel="noopener">http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg</a></li>
<li>add to project:<ul>
<li>all <code>.c</code> files</li>
<li>all <code>.cu</code> files </li>
<li>file <code>http_stream.cpp</code> from <code>\src</code> directory</li>
<li>file <code>darknet.h</code> from <code>\include</code> directory</li>
</ul>
</li>
<li>(right click on project) -&gt; properties  -&gt; Linker -&gt; General -&gt; Additional Library Directories, put here: </li>
</ul>
<p><code>C:\opencv_3.0\opencv\build\x64\vc14\lib;$(CUDA_PATH)lib\$(PlatformName);$(cudnn)\lib\x64;%(AdditionalLibraryDirectories)</code></p>
<ul>
<li>(right click on project) -&gt; properties  -&gt; Linker -&gt; Input -&gt; Additional dependecies, put here: </li>
</ul>
<p><code>..\..\3rdparty\lib\x64\pthreadVC2.lib;cublas.lib;curand.lib;cudart.lib;cudnn.lib;%(AdditionalDependencies)</code></p>
<ul>
<li>(right click on project) -&gt; properties -&gt; C/C++ -&gt; Preprocessor -&gt; Preprocessor Definitions</li>
</ul>
<p><code>OPENCV;_TIMESPEC_DEFINED;_CRT_SECURE_NO_WARNINGS;_CRT_RAND_S;WIN32;NDEBUG;_CONSOLE;_LIB;%(PreprocessorDefinitions)</code></p>
<ul>
<li><p>compile to .exe (X64 &amp; Release) and put .dll-s near with .exe: <a href="https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg" target="_blank" rel="noopener">https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg</a></p>
<ul>
<li><p><code>pthreadVC2.dll, pthreadGC2.dll</code> from \3rdparty\dll\x64</p>
</li>
<li><p><code>cusolver64_91.dll, curand64_91.dll, cudart64_91.dll, cublas64_91.dll</code> - 91 for CUDA 9.1 or your version, from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin</p>
</li>
<li><p>For OpenCV 3.2: <code>opencv_world320.dll</code> and <code>opencv_ffmpeg320_64.dll</code> from <code>C:\opencv_3.0\opencv\build\x64\vc14\bin</code> </p>
</li>
<li>For OpenCV 2.4.13: <code>opencv_core2413.dll</code>, <code>opencv_highgui2413.dll</code> and <code>opencv_ffmpeg2413_64.dll</code> from  <code>C:\opencv_2.4.13\opencv\build\x64\vc14\bin</code></li>
</ul>
</li>
</ul>
<h2 id="How-to-train-Pascal-VOC-Data"><a href="#How-to-train-Pascal-VOC-Data" class="headerlink" title="How to train (Pascal VOC Data):"></a>How to train (Pascal VOC Data):</h2><ol>
<li><p>Download pre-trained weights for the convolutional layers (154 MB): <a href="http://pjreddie.com/media/files/darknet53.conv.74" target="_blank" rel="noopener">http://pjreddie.com/media/files/darknet53.conv.74</a> and put to the directory <code>build\darknet\x64</code></p>
</li>
<li><p>Download The Pascal VOC Data and unpack it to directory <code>build\darknet\x64\data\voc</code> will be created dir <code>build\darknet\x64\data\voc\VOCdevkit\</code>:</p>
<ul>
<li><a href="http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar" target="_blank" rel="noopener">http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar</a></li>
<li><a href="http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar" target="_blank" rel="noopener">http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar</a></li>
<li><p><a href="http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar" target="_blank" rel="noopener">http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar</a></p>
<p>2.1 Download file <code>voc_label.py</code> to dir <code>build\darknet\x64\data\voc</code>: <a href="http://pjreddie.com/media/files/voc_label.py" target="_blank" rel="noopener">http://pjreddie.com/media/files/voc_label.py</a></p>
</li>
</ul>
</li>
<li><p>Download and install Python for Windows: <a href="https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe" target="_blank" rel="noopener">https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe</a></p>
</li>
<li><p>Run command: <code>python build\darknet\x64\data\voc\voc_label.py</code> (to generate files: 2007_test.txt, 2007_train.txt, 2007_val.txt, 2012_train.txt, 2012_val.txt)</p>
</li>
<li><p>Run command: <code>type 2007_train.txt 2007_val.txt 2012_*.txt &gt; train.txt</code></p>
</li>
<li><p>Set <code>batch=64</code> and <code>subdivisions=8</code> in the file <code>yolov3-voc.cfg</code>: <a href="https://github.com/AlexeyAB/darknet/blob/ee38c6e1513fb089b35be4ffa692afd9b3f65747/cfg/yolov3-voc.cfg#L3-L4" target="_blank" rel="noopener">link</a></p>
</li>
<li><p>Start training by using <code>train_voc.cmd</code> or by using the command line: </p>
<p> <code>darknet.exe detector train cfg/voc.data cfg/yolov3-voc.cfg darknet53.conv.74</code> </p>
</li>
</ol>
<p>(<strong>Note:</strong> To disable Loss-Window use flag <code>-dont_show</code>. If you are using CPU, try <code>darknet_no_gpu.exe</code> instead of <code>darknet.exe</code>.)</p>
<p>If required change pathes in the file <code>build\darknet\x64\data\voc.data</code></p>
<p>More information about training by the link: <a href="http://pjreddie.com/darknet/yolo/#train-voc" target="_blank" rel="noopener">http://pjreddie.com/darknet/yolo/#train-voc</a></p>
<p> <strong>Note:</strong> If during training you see <code>nan</code> values for <code>avg</code> (loss) field - then training goes wrong, but if <code>nan</code> is in some other lines - then training goes well.</p>
<h2 id="How-to-train-with-multi-GPU"><a href="#How-to-train-with-multi-GPU" class="headerlink" title="How to train with multi-GPU:"></a>How to train with multi-GPU:</h2><ol>
<li><p>Train it first on 1 GPU for like 1000 iterations: <code>darknet.exe detector train cfg/voc.data cfg/yolov3-voc.cfg darknet53.conv.74</code></p>
</li>
<li><p>Then stop and by using partially-trained model <code>/backup/yolov3-voc_1000.weights</code> run training with multigpu (up to 4 GPUs): <code>darknet.exe detector train cfg/voc.data cfg/yolov3-voc.cfg /backup/yolov3-voc_1000.weights -gpus 0,1,2,3</code></p>
</li>
</ol>
<p>Only for small datasets sometimes better to decrease learning rate, for 4 GPUs set <code>learning_rate = 0.00025</code> (i.e. learning_rate = 0.001 / GPUs). In this case also increase 4x times <code>burn_in =</code> and <code>max_batches =</code> in your cfg-file. I.e. use <code>burn_in = 4000</code> instead of <code>1000</code>.</p>
<p><a href="https://groups.google.com/d/msg/darknet/NbJqonJBTSY/Te5PfIpuCAAJ" target="_blank" rel="noopener">https://groups.google.com/d/msg/darknet/NbJqonJBTSY/Te5PfIpuCAAJ</a></p>
<h2 id="How-to-train-to-detect-your-custom-objects"><a href="#How-to-train-to-detect-your-custom-objects" class="headerlink" title="How to train (to detect your custom objects):"></a>How to train (to detect your custom objects):</h2><p>训练自己的模型，需要根据类别数修改yolo层的卷积核数量，注意，yolov2和yolov3计方式不一样。</p>
<p>(to train old Yolo v2 <code>yolov2-voc.cfg</code>, <code>yolov2-tiny-voc.cfg</code>, <code>yolo-voc.cfg</code>, <code>yolo-voc.2.0.cfg</code>, … <a href="https://github.com/AlexeyAB/darknet/tree/47c7af1cea5bbdedf1184963355e6418cb8b1b4f#how-to-train-pascal-voc-data" target="_blank" rel="noopener">click by the link</a>)</p>
<p>Training Yolo v3:</p>
<ol>
<li><p>Create file <code>yolo-obj.cfg</code> with the same content as in <code>yolov3.cfg</code> (or copy <code>yolov3.cfg</code> to <code>yolo-obj.cfg)</code> and:</p>
<ul>
<li>change line batch to <a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L3" target="_blank" rel="noopener"><code>batch=64</code></a></li>
<li>change line subdivisions to <a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L4" target="_blank" rel="noopener"><code>subdivisions=8</code></a></li>
<li>change line <code>classes=80</code> to your number of objects in each of 3 <code>[yolo]</code>-layers:<ul>
<li><a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L610" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L610</a></li>
<li><a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L696" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L696</a></li>
<li><a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L783" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L783</a></li>
</ul>
</li>
<li>change [<code>filters=255</code>] to filters=(classes + 5)x3 in the 3 <code>[convolutional]</code> before each <code>[yolo]</code> layer<ul>
<li><a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L603" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L603</a></li>
<li><a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L689" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L689</a></li>
<li><a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L776" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L776</a></li>
</ul>
</li>
</ul>
<p>So if <code>classes=1</code> then should be <code>filters=18</code>. If <code>classes=2</code> then write <code>filters=21</code>.</p>
<p><strong>(Do not write in the cfg-file: filters=(classes + 5)x3)</strong></p>
<p>(Generally <code>filters</code> depends on the <code>classes</code>, <code>coords</code> and number of <code>mask</code>s, i.e. filters=<code>(classes + coords + 1)*&lt;number of mask&gt;</code>, where <code>mask</code> is indices of anchors. If <code>mask</code> is absence, then filters=<code>(classes + coords + 1)*num</code>)</p>
<p>So for example, for 2 objects, your file <code>yolo-obj.cfg</code> should differ from <code>yolov3.cfg</code> in such lines in each of <strong>3</strong> [yolo]-layers:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">filters=21</span><br><span class="line"></span><br><span class="line">[region]</span><br><span class="line">classes=2</span><br></pre></td></tr></table></figure>
</li>
<li><p>Create file <code>obj.names</code> in the directory <code>build\darknet\x64\data\</code>, with objects names - each in new line</p>
</li>
<li><p>Create file <code>obj.data</code> in the directory <code>build\darknet\x64\data\</code>, containing (where <strong>classes = number of objects</strong>):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">classes= 2</span><br><span class="line">train  = data/train.txt</span><br><span class="line">valid  = data/test.txt</span><br><span class="line">names = data/obj.names</span><br><span class="line">backup = backup/</span><br></pre></td></tr></table></figure>
</li>
<li><p>Put image-files (.jpg) of your objects in the directory <code>build\darknet\x64\data\obj\</code></p>
</li>
<li><p>You should label each object on images from your dataset. Use this visual GUI-software for marking bounded boxes of objects and generating annotation files for Yolo v2 &amp; v3: <a href="https://github.com/AlexeyAB/Yolo_mark" target="_blank" rel="noopener">https://github.com/AlexeyAB/Yolo_mark</a></p>
</li>
</ol>
<p>It will create <code>.txt</code>-file for each <code>.jpg</code>-image-file - in the same directory and with the same name, but with <code>.txt</code>-extension, and put to file: object number and object coordinates on this image, for each object in new line: <code>&lt;object-class&gt; &lt;x&gt; &lt;y&gt; &lt;width&gt; &lt;height&gt;</code></p>
<p>  Where: </p>
<ul>
<li><code>&lt;object-class&gt;</code> - integer object number from <code>0</code> to <code>(classes-1)</code></li>
<li><code>&lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> - float values relative to width and height of image, it can be equal from (0.0 to 1.0]</li>
<li>for example: <code>&lt;x&gt; = &lt;absolute_x&gt; / &lt;image_width&gt;</code> or <code>&lt;height&gt; = &lt;absolute_height&gt; / &lt;image_height&gt;</code></li>
<li><p>atention: <code>&lt;x_center&gt; &lt;y_center&gt;</code> - are center of rectangle (are not top-left corner)</p>
<p>For example for <code>img1.jpg</code> you will be created <code>img1.txt</code> containing:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 0.716797 0.395833 0.216406 0.147222</span><br><span class="line">0 0.687109 0.379167 0.255469 0.158333</span><br><span class="line">1 0.420312 0.395833 0.140625 0.166667</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol start="6">
<li><p>Create file <code>train.txt</code> in directory <code>build\darknet\x64\data\</code>, with filenames of your images, each filename in new line, with path relative to <code>darknet.exe</code>, for example containing:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data/obj/img1.jpg</span><br><span class="line">data/obj/img2.jpg</span><br><span class="line">data/obj/img3.jpg</span><br></pre></td></tr></table></figure>
</li>
<li><p>Download pre-trained weights for the convolutional layers (154 MB): <a href="https://pjreddie.com/media/files/darknet53.conv.74" target="_blank" rel="noopener">https://pjreddie.com/media/files/darknet53.conv.74</a> and put to the directory <code>build\darknet\x64</code></p>
</li>
<li><p>Start training by using the command line: <code>darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74</code></p>
<p>To train on Linux use command: <code>./darknet detector train data/obj.data yolo-obj.cfg darknet53.conv.74</code> (just use <code>./darknet</code> instead of <code>darknet.exe</code>)</p>
<ul>
<li>(file <code>yolo-obj_last.weights</code> will be saved to the <code>build\darknet\x64\backup\</code> for each 100 iterations)</li>
<li>(file <code>yolo-obj_xxxx.weights</code> will be saved to the <code>build\darknet\x64\backup\</code> for each 1000 iterations)</li>
<li>(to disable Loss-Window use <code>darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -dont_show</code>, if you train on computer without monitor like a cloud Amazon EC2)</li>
<li>(to see the mAP &amp; Loss-chart during training on remote server without GUI, use command <code>darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map</code> then open URL <code>http://ip-address:8090</code> in Chrome/Firefox browser)</li>
</ul>
</li>
</ol>
<p>8.1. For training with mAP (mean average precisions) calculation for each 4 Epochs (set <code>valid=valid.txt</code> or <code>train.txt</code> in <code>obj.data</code> file) and run: <code>darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -map</code></p>
<ol start="9">
<li><p>After training is complete - get result <code>yolo-obj_final.weights</code> from path <code>build\darknet\x64\backup\</code></p>
<ul>
<li><p>After each 100 iterations you can stop and later start training from this point. For example, after 2000 iterations you can stop training, and later just copy <code>yolo-obj_2000.weights</code> from <code>build\darknet\x64\backup\</code> to <code>build\darknet\x64\</code> and start training using: <code>darknet.exe detector train data/obj.data yolo-obj.cfg yolo-obj_2000.weights</code></p>
<p>(in the original repository <a href="https://github.com/pjreddie/darknet" target="_blank" rel="noopener">https://github.com/pjreddie/darknet</a> the weights-file is saved only once every 10 000 iterations <code>if(iterations &gt; 1000)</code>)</p>
</li>
<li><p>Also you can get result earlier than all 45000 iterations.</p>
</li>
</ul>
<p><strong>Note:</strong> If during training you see <code>nan</code> values for <code>avg</code> (loss) field - then training goes wrong, but if <code>nan</code> is in some other lines - then training goes well.</p>
<p><strong>Note:</strong> If you changed width= or height= in your cfg-file, then new width and height must be divisible by 32.</p>
<p><strong>Note:</strong> After training use such command for detection: <code>darknet.exe detector test data/obj.data yolo-obj.cfg yolo-obj_8000.weights</code></p>
<p><strong>Note:</strong> if error <code>Out of memory</code> occurs then in <code>.cfg</code>-file you should increase <code>subdivisions=16</code>, 32 or 64: <a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L4" target="_blank" rel="noopener">link</a></p>
</li>
</ol>
<h3 id="How-to-train-tiny-yolo-to-detect-your-custom-objects"><a href="#How-to-train-tiny-yolo-to-detect-your-custom-objects" class="headerlink" title="How to train tiny-yolo (to detect your custom objects):"></a>How to train tiny-yolo (to detect your custom objects):</h3><p>Do all the same steps as for the full yolo model as described above. With the exception of:</p>
<ul>
<li>Download default weights file for yolov3-tiny: <a href="https://pjreddie.com/media/files/yolov3-tiny.weights" target="_blank" rel="noopener">https://pjreddie.com/media/files/yolov3-tiny.weights</a></li>
<li>Get pre-trained weights <code>yolov3-tiny.conv.15</code> using command: <code>darknet.exe partial cfg/yolov3-tiny.cfg yolov3-tiny.weights yolov3-tiny.conv.15 15</code></li>
<li>Make your custom model <code>yolov3-tiny-obj.cfg</code> based on <code>cfg/yolov3-tiny_obj.cfg</code> instead of <code>yolov3.cfg</code></li>
<li>Start training: <code>darknet.exe detector train data/obj.data yolov3-tiny-obj.cfg yolov3-tiny.conv.15</code></li>
</ul>
<p>For training Yolo based on other models (<a href="https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/densenet201_yolo.cfg" target="_blank" rel="noopener">DenseNet201-Yolo</a> or <a href="https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/resnet50_yolo.cfg" target="_blank" rel="noopener">ResNet50-Yolo</a>), you can download and get pre-trained weights as showed in this file: <a href="https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/partial.cmd" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/partial.cmd</a><br>If you made you custom model that isn’t based on other models, then you can train it without pre-trained weights, then will be used random initial weights.</p>
<h2 id="When-should-I-stop-training"><a href="#When-should-I-stop-training" class="headerlink" title="When should I stop training:"></a>When should I stop training:</h2><p>Usually sufficient 2000 iterations for each class(object), but not less than 4000 iterations in total. But for a more precise definition when you should stop training, use the following manual:</p>
<ol>
<li><p>During training, you will see varying indicators of error, and you should stop when no longer decreases <strong>0.XXXXXXX avg</strong>:</p>
<blockquote>
<p>Region Avg IOU: 0.798363, Class: 0.893232, Obj: 0.700808, No Obj: 0.004567, Avg Recall: 1.000000,  count: 8<br>Region Avg IOU: 0.800677, Class: 0.892181, Obj: 0.701590, No Obj: 0.004574, Avg Recall: 1.000000,  count: 8</p>
<p><strong>9002</strong>: 0.211667, <strong>0.060730 avg</strong>, 0.001000 rate, 3.868000 seconds, 576128 images<br>Loaded: 0.000000 seconds</p>
</blockquote>
<ul>
<li><strong>9002</strong> - iteration number (number of batch)</li>
<li><strong>0.060730 avg</strong> - average loss (error) - <strong>the lower, the better</strong></li>
</ul>
<p>When you see that average loss <strong>0.xxxxxx avg</strong> no longer decreases at many iterations then you should stop training.</p>
</li>
<li><p>Once training is stopped, you should take some of last <code>.weights</code>-files from <code>darknet\build\darknet\x64\backup</code> and choose the best of them:</p>
</li>
</ol>
<p>For example, you stopped training after 9000 iterations, but the best result can give one of previous weights (7000, 8000, 9000). It can happen due to overfitting. <strong>Overfitting</strong> - is case when you can detect objects on images from training-dataset, but can’t detect objects on any others images. You should get weights from <strong>Early Stopping Point</strong>:</p>
<p><img src="https://hsto.org/files/5dc/7ae/7fa/5dc7ae7fad9d4e3eb3a484c58bfc1ff5.png" alt="Overfitting"> </p>
<p>To get weights from Early Stopping Point:</p>
<p>  2.1. At first, in your file <code>obj.data</code> you must specify the path to the validation dataset <code>valid = valid.txt</code> (format of <code>valid.txt</code> as in <code>train.txt</code>), and if you haven’t validation images, just copy <code>data\train.txt</code> to <code>data\valid.txt</code>.</p>
<p>  2.2 If training is stopped after 9000 iterations, to validate some of previous weights use this commands:</p>
<p>(If you use another GitHub repository, then use <code>darknet.exe detector recall</code>… instead of <code>darknet.exe detector map</code>…)</p>
<ul>
<li><code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights</code></li>
<li><code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_8000.weights</code></li>
<li><code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_9000.weights</code></li>
</ul>
<p>And comapre last output lines for each weights (7000, 8000, 9000):</p>
<p>Choose weights-file <strong>with the highest mAP (mean average precision)</strong> or IoU (intersect over union)</p>
<p>For example, <strong>bigger mAP</strong> gives weights <code>yolo-obj_8000.weights</code> - then <strong>use this weights for detection</strong>.</p>
<p>Or just train with <code>-map</code> flag: </p>
<p><code>darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -map</code> </p>
<p>So you will see mAP-chart (red-line) in the Loss-chart Window. mAP will be calculated for each 4 Epochs using <code>valid=valid.txt</code> file that is specified in <code>obj.data</code> file (<code>1 Epoch = images_in_train_txt / batch</code> iterations)</p>
<p><img src="https://hsto.org/webt/yd/vl/ag/ydvlagutof2zcnjodstgroen8ac.jpeg" alt="loss_chart_map_chart"></p>
<p>Example of custom object detection: <code>darknet.exe detector test data/obj.data yolo-obj.cfg yolo-obj_8000.weights</code></p>
<ul>
<li><p><strong>IoU</strong> (intersect over union) - average instersect over union of objects and detections for a certain threshold = 0.24</p>
</li>
<li><p><strong>mAP</strong> (mean average precision) - mean value of <code>average precisions</code> for each class, where <code>average precision</code> is average value of 11 points on PR-curve for each possible threshold (each probability of detection) for the same class (Precision-Recall in terms of PascalVOC, where Precision=TP/(TP+FP) and Recall=TP/(TP+FN) ), page-11: <a href="http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf" target="_blank" rel="noopener">http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf</a></p>
</li>
</ul>
<p><strong>mAP</strong> is default metric of precision in the PascalVOC competition, <strong>this is the same as AP50</strong> metric in the MS COCO competition.<br>In terms of Wiki, indicators Precision and Recall have a slightly different meaning than in the PascalVOC competition, but <strong>IoU always has the same meaning</strong>.</p>
<p><img src="https://hsto.org/files/ca8/866/d76/ca8866d76fb840228940dbf442a7f06a.jpg" alt="precision_recall_iou"></p>
<h3 id="How-to-calculate-mAP-on-PascalVOC-2007"><a href="#How-to-calculate-mAP-on-PascalVOC-2007" class="headerlink" title="How to calculate mAP on PascalVOC 2007:"></a>How to calculate mAP on PascalVOC 2007:</h3><ol>
<li>To calculate mAP (mean average precision) on PascalVOC-2007-test:</li>
</ol>
<ul>
<li>Download PascalVOC dataset, install Python 3.x and get file <code>2007_test.txt</code> as described here: <a href="https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data</a></li>
<li>Then download file <a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/scripts/voc_label_difficult.py" target="_blank" rel="noopener">https://raw.githubusercontent.com/AlexeyAB/darknet/master/scripts/voc_label_difficult.py</a> to the dir <code>build\darknet\x64\data\</code> then run <code>voc_label_difficult.py</code> to get the file <code>difficult_2007_test.txt</code></li>
<li>Remove symbol <code>#</code> from this line to un-comment it: <a href="https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/data/voc.data#L4" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/data/voc.data#L4</a></li>
<li><p>Then there are 2 ways to get mAP:</p>
<ol>
<li>Using Darknet + Python: run the file <code>build/darknet/x64/calc_mAP_voc_py.cmd</code> - you will get mAP for <code>yolo-voc.cfg</code> model, mAP = 75.9%</li>
<li>Using this fork of Darknet: run the file <code>build/darknet/x64/calc_mAP.cmd</code> - you will get mAP for <code>yolo-voc.cfg</code> model, mAP = 75.8%</li>
</ol>
<p>(The article specifies the value of mAP = 76.8% for YOLOv2 416×416, page-4 table-3: <a href="https://arxiv.org/pdf/1612.08242v1.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1612.08242v1.pdf</a>. We get values lower - perhaps due to the fact that the model was trained on a slightly different source code than the code on which the detection is was done)</p>
</li>
<li><p>if you want to get mAP for <code>tiny-yolo-voc.cfg</code> model, then un-comment line for tiny-yolo-voc.cfg and comment line for yolo-voc.cfg in the .cmd-file</p>
</li>
<li>if you have Python 2.x instead of Python 3.x, and if you use Darknet+Python-way to get mAP, then in your cmd-file use <code>reval_voc.py</code> and <code>voc_eval.py</code> instead of <code>reval_voc_py3.py</code> and <code>voc_eval_py3.py</code> from this directory: <a href="https://github.com/AlexeyAB/darknet/tree/master/scripts" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/tree/master/scripts</a></li>
</ul>
<h3 id="Custom-object-detection"><a href="#Custom-object-detection" class="headerlink" title="Custom object detection:"></a>Custom object detection:</h3><p>Example of custom object detection: <code>darknet.exe detector test data/obj.data yolo-obj.cfg yolo-obj_8000.weights</code></p>
<table>
<thead>
<tr>
<th><img src="https://hsto.org/files/d12/1e7/515/d121e7515f6a4eb694913f10de5f2b61.jpg" alt="Yolo_v2_training"></th>
<th><img src="https://hsto.org/files/727/c7e/5e9/727c7e5e99bf4d4aa34027bb6a5e4bab.jpg" alt="Yolo_v2_training"></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<h2 id="How-to-improve-object-detection"><a href="#How-to-improve-object-detection" class="headerlink" title="How to improve object detection:"></a>How to improve object detection:</h2><ol>
<li><p>Before training:</p>
<ul>
<li><p>set flag <code>random=1</code> in your <code>.cfg</code>-file - it will increase precision by training Yolo for different resolutions: <a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L788" target="_blank" rel="noopener">link</a></p>
</li>
<li><p>increase network resolution in your <code>.cfg</code>-file (<code>height=608</code>, <code>width=608</code> or any value multiple of 32) - it will increase precision</p>
</li>
</ul>
</li>
</ol>
<ul>
<li><p>check that each object are mandatory labeled in your dataset - no one object in your data set should not be without label. In the most training issues - there are wrong labels in your dataset (got labels by using some conversion script, marked with a third-party tool, …). Always check your dataset by using: <a href="https://github.com/AlexeyAB/Yolo_mark" target="_blank" rel="noopener">https://github.com/AlexeyAB/Yolo_mark</a></p>
</li>
<li><p>for each object which you want to detect - there must be at least 1 similar object in the Training dataset with about the same: shape, side of object, relative size, angle of rotation, tilt, illumination. So desirable that your training dataset include images with objects at diffrent: scales, rotations, lightings, from different sides, on different backgrounds - you should preferably have 2000 different images for each class or more, and you should train <code>2000*classes</code> iterations or more</p>
</li>
<li><p>desirable that your training dataset include images with non-labeled objects that you do not want to detect - negative samples without bounded box (empty <code>.txt</code> files) - use as many images of negative samples as there are images with objects</p>
</li>
<li><p>for training with a large number of objects in each image, add the parameter <code>max=200</code> or higher value in the last <code>[yolo]</code>-layer or <code>[region]</code>-layer in your cfg-file (the global maximum number of objects that can be detected by YoloV3 is <code>0,0615234375*(width*height)</code> where are width and height are parameters from <code>[net]</code> section in cfg-file) </p>
</li>
<li><p>for training for small objects (smaller than 16x16 after the image is resized to 416x416) - set <code>layers = -1, 11</code> instead of <a href="https://github.com/AlexeyAB/darknet/blob/6390a5a2ab61a0bdf6f1a9a6b4a739c16b36e0d7/cfg/yolov3.cfg#L720" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/6390a5a2ab61a0bdf6f1a9a6b4a739c16b36e0d7/cfg/yolov3.cfg#L720</a><br>  and set <code>stride=4</code> instead of <a href="https://github.com/AlexeyAB/darknet/blob/6390a5a2ab61a0bdf6f1a9a6b4a739c16b36e0d7/cfg/yolov3.cfg#L717" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/6390a5a2ab61a0bdf6f1a9a6b4a739c16b36e0d7/cfg/yolov3.cfg#L717</a></p>
</li>
<li><p>for training for both small and large objects use modified models:</p>
<ul>
<li>Full-model: 5 yolo layers: <a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3_5l.cfg" target="_blank" rel="noopener">https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3_5l.cfg</a></li>
<li>Tiny-model: 3 yolo layers: <a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3-tiny_3l.cfg" target="_blank" rel="noopener">https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3-tiny_3l.cfg</a></li>
<li>Spatial-full-model: 3 yolo layers: <a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3-spp.cfg" target="_blank" rel="noopener">https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3-spp.cfg</a></li>
</ul>
</li>
<li><p>If you train the model to distinguish Left and Right objects as separate classes (left/right hand, left/right-turn on road signs, …) then for disabling flip data augmentation - add <code>flip=0</code> here: <a href="https://github.com/AlexeyAB/darknet/blob/3d2d0a7c98dbc8923d9ff705b81ff4f7940ea6ff/cfg/yolov3.cfg#L17" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/3d2d0a7c98dbc8923d9ff705b81ff4f7940ea6ff/cfg/yolov3.cfg#L17</a></p>
</li>
<li><p>General rule - your training dataset should include such a set of relative sizes of objects that you want to detect: </p>
<ul>
<li><code>train_network_width * train_obj_width / train_image_width ~= detection_network_width * detection_obj_width / detection_image_width</code></li>
<li><code>train_network_height * train_obj_height / train_image_height ~= detection_network_height * detection_obj_height / detection_image_height</code></li>
</ul>
<p>I.e. for each object from Test dataset there must be at least 1 object in the Training dataset with the same class_id and about the same relative size:</p>
<p><code>object width in percent from Training dataset</code> ~= <code>object width in percent from Test dataset</code> </p>
<p>That is, if only objects that occupied 80-90% of the image were present in the training set, then the trained network will not be able to detect objects that occupy 1-10% of the image.</p>
</li>
<li><p>to speedup training (with decreasing detection accuracy) do Fine-Tuning instead of Transfer-Learning, set param <code>stopbackward=1</code> here: <a href="https://github.com/AlexeyAB/darknet/blob/6d44529cf93211c319813c90e0c1adb34426abe5/cfg/yolov3.cfg#L548" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/6d44529cf93211c319813c90e0c1adb34426abe5/cfg/yolov3.cfg#L548</a><br>then do this command: <code>./darknet partial cfg/yolov3.cfg yolov3.weights yolov3.conv.81 81</code> will be created file <code>yolov3.conv.81</code>,<br>then train by using weights file <code>yolov3.conv.81</code> instead of <code>darknet53.conv.74</code></p>
</li>
<li><p>The more different objects you want to detect, the more complex network model should be used. But each: <code>model of object, side, illimination, scale, each 30 grad</code> of the turn and inclination angles - these are different objects from a neural network perspective.</p>
</li>
<li><p>recalculate anchors for your dataset for <code>width</code> and <code>height</code> from cfg-file:<br><code>darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416</code><br>then set the same 9 <code>anchors</code> in each of 3 <code>[yolo]</code>-layers in your cfg-file. But you should change indexes of anchors <code>masks=</code> for each [yolo]-layer, so that 1st-[yolo]-layer has anchors larger than 60x60, 2nd larger than 30x30, 3rd remaining. If many of the calculated anchors do not fit under the appropriate layers - then just try using all the default anchors.</p>
</li>
</ul>
<ol start="2">
<li><p>After training - for detection:</p>
<ul>
<li><p>Increase network-resolution by set in your <code>.cfg</code>-file (<code>height=608</code> and <code>width=608</code>) or (<code>height=832</code> and <code>width=832</code>) or (any value multiple of 32) - this increases the precision and makes it possible to detect small objects: <a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L8-L9" target="_blank" rel="noopener">link</a></p>
<ul>
<li>it is not necessary to train the network again, just use <code>.weights</code>-file already trained for 416x416 resolution</li>
<li>but to get even greater accuracy you should train with higher resolution 608x608 or 832x832, note: if error <code>Out of memory</code> occurs then in <code>.cfg</code>-file you should increase <code>subdivisions=16</code>, 32 or 64: <a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L4" target="_blank" rel="noopener">link</a></li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="How-to-mark-bounded-boxes-of-objects-and-create-annotation-files"><a href="#How-to-mark-bounded-boxes-of-objects-and-create-annotation-files" class="headerlink" title="How to mark bounded boxes of objects and create annotation files:"></a>How to mark bounded boxes of objects and create annotation files:</h2><p>Here you can find repository with GUI-software for marking bounded boxes of objects and generating annotation files for Yolo v2 &amp; v3: <a href="https://github.com/AlexeyAB/Yolo_mark" target="_blank" rel="noopener">https://github.com/AlexeyAB/Yolo_mark</a></p>
<p>With example of: <code>train.txt</code>, <code>obj.names</code>, <code>obj.data</code>, <code>yolo-obj.cfg</code>, <code>air</code>1-6<code>.txt</code>, <code>bird</code>1-4<code>.txt</code> for 2 classes of objects (air, bird) and <code>train_obj.cmd</code> with example how to train this image-set with Yolo v2 &amp; v3</p>
<h2 id="Using-Yolo9000"><a href="#Using-Yolo9000" class="headerlink" title="Using Yolo9000"></a>Using Yolo9000</h2><p> Simultaneous detection and classification of 9000 objects: <code>darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights data/dog.jpg</code></p>
<ul>
<li><p><code>yolo9000.weights</code> - (186 MB Yolo9000 Model) requires 4 GB GPU-RAM: <a href="http://pjreddie.com/media/files/yolo9000.weights" target="_blank" rel="noopener">http://pjreddie.com/media/files/yolo9000.weights</a></p>
</li>
<li><p><code>yolo9000.cfg</code> - cfg-file of the Yolo9000, also there are paths to the <code>9k.tree</code> and <code>coco9k.map</code>  <a href="https://github.com/AlexeyAB/darknet/blob/617cf313ccb1fe005db3f7d88dec04a04bd97cc2/cfg/yolo9000.cfg#L217-L218" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/617cf313ccb1fe005db3f7d88dec04a04bd97cc2/cfg/yolo9000.cfg#L217-L218</a></p>
<ul>
<li><p><code>9k.tree</code> - <strong>WordTree</strong> of 9418 categories  - <code>&lt;label&gt; &lt;parent_it&gt;</code>, if <code>parent_id == -1</code> then this label hasn’t parent: <a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/9k.tree" target="_blank" rel="noopener">https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/9k.tree</a></p>
</li>
<li><p><code>coco9k.map</code> - map 80 categories from MSCOCO to WordTree <code>9k.tree</code>: <a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/coco9k.map" target="_blank" rel="noopener">https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/coco9k.map</a></p>
</li>
</ul>
</li>
<li><p><code>combine9k.data</code> - data file, there are paths to: <code>9k.labels</code>, <code>9k.names</code>, <code>inet9k.map</code>, (change path to your <code>combine9k.train.list</code>): <a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/combine9k.data" target="_blank" rel="noopener">https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/combine9k.data</a></p>
<ul>
<li><p><code>9k.labels</code> - 9418 labels of objects: <a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/9k.labels" target="_blank" rel="noopener">https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/9k.labels</a></p>
</li>
<li><p><code>9k.names</code> -<br>9418 names of objects: <a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/9k.names" target="_blank" rel="noopener">https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/9k.names</a></p>
</li>
<li><p><code>inet9k.map</code> - map 200 categories from ImageNet to WordTree <code>9k.tree</code>: <a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/inet9k.map" target="_blank" rel="noopener">https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/inet9k.map</a></p>
</li>
</ul>
</li>
</ul>
<h2 id="How-to-use-Yolo-as-DLL-and-SO-libraries"><a href="#How-to-use-Yolo-as-DLL-and-SO-libraries" class="headerlink" title="How to use Yolo as DLL and SO libraries"></a>How to use Yolo as DLL and SO libraries</h2><ul>
<li>on Linux - set <code>LIBSO=1</code> in the <code>Makefile</code> and do <code>make</code></li>
<li>on Windows - compile <code>build\darknet\yolo_cpp_dll.sln</code> or <code>build\darknet\yolo_cpp_dll_no_gpu.sln</code> solution</li>
</ul>
<p>There are 2 APIs:</p>
<ul>
<li><p>C API: <a href="https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h</a></p>
<ul>
<li>Python examples using the C API::     <ul>
<li><a href="https://github.com/AlexeyAB/darknet/blob/master/darknet.py" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/master/darknet.py</a>    </li>
<li><a href="https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py</a></li>
</ul>
</li>
</ul>
</li>
<li><p>C++ API: <a href="https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp</a></p>
<ul>
<li>C++ example that uses C++ API: <a href="https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp</a></li>
</ul>
</li>
</ul>
<hr>
<ol>
<li><p>To compile Yolo as C++ DLL-file <code>yolo_cpp_dll.dll</code> - open in MSVS2015 file <code>build\darknet\yolo_cpp_dll.sln</code>, set <strong>x64</strong> and <strong>Release</strong>, and do the: Build -&gt; Build yolo_cpp_dll</p>
<ul>
<li>You should have installed <strong>CUDA 10.0</strong></li>
<li>To use cuDNN do: (right click on project) -&gt; properties -&gt; C/C++ -&gt; Preprocessor -&gt; Preprocessor Definitions, and add at the beginning of line: <code>CUDNN;</code></li>
</ul>
</li>
<li><p>To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file <code>build\darknet\yolo_console_dll.sln</code>, set <strong>x64</strong> and <strong>Release</strong>, and do the: Build -&gt; Build yolo_console_dll</p>
<ul>
<li><p>you can run your console application from Windows Explorer <code>build\darknet\x64\yolo_console_dll.exe</code><br><strong>use this command</strong>: <code>yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4</code></p>
</li>
<li><p>after launching your console application and entering the image file name - you will see info for each object:<br><code>&lt;obj_id&gt; &lt;left_x&gt; &lt;top_y&gt; &lt;width&gt; &lt;height&gt; &lt;probability&gt;</code></p>
</li>
<li>to use simple OpenCV-GUI you should uncomment line <code>//#define OPENCV</code> in <code>yolo_console_dll.cpp</code>-file: <a href="https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5" target="_blank" rel="noopener">link</a></li>
<li>you can see source code of simple example for detection on the video file: <a href="https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75" target="_blank" rel="noopener">link</a></li>
</ul>
</li>
</ol>
<p><code>yolo_cpp_dll.dll</code>-API: <a href="https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42" target="_blank" rel="noopener">link</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">struct bbox_t &#123;</span><br><span class="line">    unsigned int x, y, w, h;    // (x,y) - top-left corner, (w, h) - width &amp; height of bounded box</span><br><span class="line">    float prob;                    // confidence - probability that the object was found correctly</span><br><span class="line">    unsigned int obj_id;        // class of object - from range [0, classes-1]</span><br><span class="line">    unsigned int track_id;        // tracking id for video (0 - untracked, 1 - inf - tracked object)</span><br><span class="line">    unsigned int frames_counter;// counter of frames on which the object was detected</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">class Detector &#123;</span><br><span class="line">public:</span><br><span class="line">        Detector(std::string cfg_filename, std::string weight_filename, int gpu_id = 0);</span><br><span class="line">        ~Detector();</span><br><span class="line"></span><br><span class="line">        std::vector&lt;bbox_t&gt; detect(std::string image_filename, float thresh = 0.2, bool use_mean = false);</span><br><span class="line">        std::vector&lt;bbox_t&gt; detect(image_t img, float thresh = 0.2, bool use_mean = false);</span><br><span class="line">        static image_t load_image(std::string image_filename);</span><br><span class="line">        static void free_image(image_t m);</span><br><span class="line"></span><br><span class="line">#ifdef OPENCV</span><br><span class="line">        std::vector&lt;bbox_t&gt; detect(cv::Mat mat, float thresh = 0.2, bool use_mean = false);</span><br><span class="line">	std::shared_ptr&lt;image_t&gt; mat_to_image_resize(cv::Mat mat) const;</span><br><span class="line">#endif</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/02/15/博文0002/" data-id="cjs5rat840004lbrhivtasgtv" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/yolo-darknet/">yolo-darknet</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2019/02/15/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/心得分享/">心得分享</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/yolo-darknet/">yolo-darknet</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/yolo-darknet/" style="font-size: 10px;">yolo-darknet</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/02/15/博文0002/">博文0002</a>
          </li>
        
          <li>
            <a href="/2019/02/15/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2019/02/06/博文0001/">博文0001</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>